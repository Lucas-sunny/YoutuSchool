"""
Google Trends Crawler - The Info Club v2.0
pytrendsë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ íŠœë²„ ê´€ë ¨ í‚¤ì›Œë“œì˜ ê²€ìƒ‰ íŠ¸ë Œë“œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""
import os
import requests
import json
from dotenv import load_dotenv
from datetime import datetime, date
import time as time_module

load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# ìœ íŠœë²„ì—ê²Œ ì¤‘ìš”í•œ ì‹œë“œ í‚¤ì›Œë“œ (ì¹´í…Œê³ ë¦¬ë³„)
SEED_KEYWORDS = {
    "ì½˜í…ì¸  íŠ¸ë Œë“œ": ["ìœ íŠœë¸Œ ì‡¼ì¸ ", "ë¸Œì´ë¡œê·¸", "ë¨¹ë°©", "ASMR", "ì–¸ë°•ì‹±"],
    "í”Œë«í¼ íŠ¸ë Œë“œ": ["ìœ íŠœë¸Œ", "í‹±í†¡", "ì¸ìŠ¤íƒ€ ë¦´ìŠ¤", "íŠ¸ìœ„ì¹˜", "AI ì˜ìƒ"],
    "í¬ë¦¬ì—ì´í„° ë„êµ¬": ["ì˜ìƒ í¸ì§‘", "ì¸ë„¤ì¼", "ìë§‰ ìƒì„±", "AI ë”ë¹™", "SEO"],
    "ì¸ê¸° ì£¼ì œ": ["ê²Œì„", "K-POP", "ì—¬í–‰ ë¸Œì´ë¡œê·¸", "ì¬í…Œí¬", "ìê¸°ê³„ë°œ"],
}


def get_supabase_headers():
    return {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json",
        "Prefer": "resolution=merge-duplicates"
    }


def fetch_keyword_trends(keywords, category_name, geo="KR"):
    """
    pytrends interest_over_timeìœ¼ë¡œ í‚¤ì›Œë“œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        keywords: ë¶„ì„í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 5ê°œ)
        category_name: í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ì´ë¦„
        geo: êµ­ê°€ ì½”ë“œ
    
    Returns:
        list of keyword trend dicts
    """
    try:
        from pytrends.request import TrendReq

        pytrends = TrendReq(hl='ko', tz=540)
        pytrends.build_payload(keywords[:5], timeframe='now 7-d', geo=geo)
        
        df = pytrends.interest_over_time()
        
        if df.empty:
            print(f"    âš ï¸ '{category_name}' ì¹´í…Œê³ ë¦¬ ë°ì´í„° ì—†ìŒ")
            return []

        results = []
        for keyword in keywords[:5]:
            if keyword not in df.columns:
                continue
            
            avg_interest = int(df[keyword].mean())
            max_interest = int(df[keyword].max())
            latest = int(df[keyword].iloc[-1])
            
            # íŠ¸ë Œë“œ ë°©í–¥ íŒë‹¨: ìµœê·¼ vs í‰ê· 
            if latest > avg_interest * 1.3:
                trend_direction = "ğŸ“ˆ ê¸‰ìƒìŠ¹"
            elif latest > avg_interest:
                trend_direction = "â†—ï¸ ìƒìŠ¹"
            elif latest < avg_interest * 0.7:
                trend_direction = "ğŸ“‰ í•˜ë½"
            else:
                trend_direction = "â†’ ë³´í•©"
            
            traffic_info = f"{trend_direction} (í˜„ì¬:{latest}, í‰ê· :{avg_interest}, ìµœê³ :{max_interest})"
            
            results.append({
                "keyword": keyword,
                "region": "KR" if geo == "KR" else "US",
                "traffic_volume": traffic_info,
                "related_topics": category_name,
                "trending_date": date.today().isoformat(),
                "crawled_at": datetime.now().isoformat()
            })

        print(f"    âœ… '{category_name}': {len(results)}ê°œ í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ")
        return results

    except Exception as e:
        print(f"    âŒ Trends Error ({category_name}): {e}")
        return []


def fetch_related_queries(seed_keywords, geo="KR"):
    """íŠ¹ì • í‚¤ì›Œë“œì˜ ê´€ë ¨ ê²€ìƒ‰ì–´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        from pytrends.request import TrendReq

        pytrends = TrendReq(hl='ko', tz=540)
        pytrends.build_payload(seed_keywords[:5], timeframe='now 7-d', geo=geo)

        related = pytrends.related_queries()
        result = {}

        for keyword, data in related.items():
            top_queries = []
            if data.get("top") is not None and not data["top"].empty:
                top_queries = data["top"]["query"].tolist()[:10]
            result[keyword] = top_queries

        return result

    except Exception as e:
        print(f"    âŒ Related queries error: {e}")
        return {}


def save_to_supabase(keywords):
    """ìˆ˜ì§‘í•œ í‚¤ì›Œë“œë¥¼ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤."""
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("  âš ï¸ Supabase credentials not set.")
        return

    endpoint = f"{SUPABASE_URL}/rest/v1/google_trends"
    headers = get_supabase_headers()

    saved = 0
    for kw in keywords:
        try:
            # ê¸°ì¡´ ë™ì¼ í‚¤ì›Œë“œ + ë‚ ì§œ ë°ì´í„° í™•ì¸ í›„ upsert
            check_url = f"{SUPABASE_URL}/rest/v1/google_trends?keyword=eq.{kw['keyword']}&trending_date=eq.{kw['trending_date']}&region=eq.{kw['region']}"
            check_resp = requests.get(check_url, headers=get_supabase_headers())
            
            if check_resp.status_code == 200 and len(check_resp.json()) > 0:
                # ì—…ë°ì´íŠ¸
                existing_id = check_resp.json()[0]['id']
                update_url = f"{SUPABASE_URL}/rest/v1/google_trends?id=eq.{existing_id}"
                resp = requests.patch(update_url, json=kw, headers=headers)
            else:
                # ìƒˆë¡œ ì‚½ì…
                resp = requests.post(endpoint, json=kw, headers=headers)
            
            if resp.status_code in range(200, 300):
                saved += 1
            else:
                pass  # ì¡°ìš©íˆ ë„˜ì–´ê°
        except Exception as e:
            print(f"    âŒ Save error: {e}")

    print(f"  ğŸ’¾ {saved}/{len(keywords)}ê°œ ì €ì¥ ì™„ë£Œ")


def run_google_trends_crawler():
    """Google Trends í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    print(f"\n[{datetime.now()}] ğŸ“Š Google Trends Crawler ì‹œì‘...")

    all_keywords = []
    
    for category_name, keywords in SEED_KEYWORDS.items():
        print(f"  ğŸ“ '{category_name}' ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì¤‘...")
        trends = fetch_keyword_trends(keywords, category_name, geo="KR")
        all_keywords.extend(trends)
        time_module.sleep(2)  # Google rate limit ë°©ì§€

    # ê´€ë ¨ ê²€ìƒ‰ì–´ë„ ìˆ˜ì§‘ (ìƒìœ„ ì¹´í…Œê³ ë¦¬ ëŒ€í‘œ í‚¤ì›Œë“œ)
    print(f"  ğŸ” ê´€ë ¨ ê²€ìƒ‰ì–´ ë¶„ì„ ì¤‘...")
    top_keywords = ["ìœ íŠœë¸Œ", "ì‡¼ì¸ ", "AI"]
    related = fetch_related_queries(top_keywords)
    
    for main_kw, related_list in related.items():
        for rq in related_list[:5]:
            all_keywords.append({
                "keyword": rq,
                "region": "KR",
                "traffic_volume": f"'{main_kw}' ê´€ë ¨ ê²€ìƒ‰ì–´",
                "related_topics": f"{main_kw} ê´€ë ¨",
                "trending_date": date.today().isoformat(),
                "crawled_at": datetime.now().isoformat()
            })

    if all_keywords:
        save_to_supabase(all_keywords)
        
        # ì½˜ì†” ë¯¸ë¦¬ë³´ê¸°
        print("\n  ğŸ“Š ìˆ˜ì§‘ëœ í‚¤ì›Œë“œ ë¯¸ë¦¬ë³´ê¸°:")
        for kw in all_keywords[:10]:
            print(f"    â€¢ {kw['keyword']}: {kw['traffic_volume']}")
    else:
        print("  âš ï¸ ìˆ˜ì§‘ëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

    print(f"  ğŸ“Š Google Trends Crawler ì™„ë£Œ! (ì´ {len(all_keywords)}ê°œ)\n")
    return all_keywords


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--once":
        print("ğŸš€ Google Trends Crawler (ë‹¨ì¼ ì‹¤í–‰)...")
        run_google_trends_crawler()
        print("âœ… ì™„ë£Œ!")
    else:
        print("ğŸš€ Google Trends Crawler (ë°˜ë³µ ëª¨ë“œ, 6ì‹œê°„ ê°„ê²©)")
        while True:
            run_google_trends_crawler()
            print("ğŸ˜´ 6ì‹œê°„ ëŒ€ê¸° ì¤‘...")
            time_module.sleep(21600)  # 6ì‹œê°„ ê°„ê²©
