"""
Cross-Platform Trend Analyzer - The Info Club v2.0
Reddit + YouTube + Google Trends ë°ì´í„°ë¥¼ êµì°¨ ë¶„ì„í•˜ì—¬ ì£¼ê°„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
OpenAI REST APIë¥¼ requestsë¡œ ì§ì ‘ í˜¸ì¶œ (Python 3.14 í˜¸í™˜)
"""
import os
import json
import requests
from dotenv import load_dotenv
from datetime import datetime, date, timedelta

load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def get_supabase_headers():
    return {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json",
        "Prefer": "resolution=merge-duplicates"
    }


def call_openai(system_prompt, user_prompt, max_tokens=1500):
    """OpenAI REST APIë¥¼ requestsë¡œ ì§ì ‘ í˜¸ì¶œí•©ë‹ˆë‹¤."""
    if not OPENAI_API_KEY:
        print("  âš ï¸ OPENAI_API_KEY not set.")
        return None

    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "gpt-4o-mini",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "max_tokens": max_tokens,
        "temperature": 0.7
    }

    try:
        resp = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        if resp.status_code == 200:
            return resp.json()["choices"][0]["message"]["content"].strip()
        else:
            print(f"  âŒ OpenAI Error: {resp.status_code} - {resp.text[:200]}")
            return None
    except Exception as e:
        print(f"  âŒ OpenAI Exception: {e}")
        return None


def fetch_recent_reddit_posts(days=7):
    """ìµœê·¼ Nì¼ê°„ Reddit í¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
    since = (datetime.now() - timedelta(days=days)).isoformat()
    url = f"{SUPABASE_URL}/rest/v1/posts?crawled_at=gte.{since}&select=title,subreddit,ai_insight&order=crawled_at.desc&limit=50"
    resp = requests.get(url, headers=get_supabase_headers())
    if resp.status_code == 200:
        return resp.json()
    return []


def fetch_recent_youtube_trends(days=7):
    """ìµœê·¼ Nì¼ê°„ YouTube íŠ¸ë Œë”© ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    since = date.today() - timedelta(days=days)
    url = f"{SUPABASE_URL}/rest/v1/youtube_trends?trending_date=gte.{since.isoformat()}&select=title,channel_title,category,view_count,region&order=view_count.desc&limit=50"
    resp = requests.get(url, headers=get_supabase_headers())
    if resp.status_code == 200:
        return resp.json()
    return []


def fetch_recent_google_trends(days=7):
    """ìµœê·¼ Nì¼ê°„ Google íŠ¸ë Œë“œ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°"""
    since = date.today() - timedelta(days=days)
    url = f"{SUPABASE_URL}/rest/v1/google_trends?trending_date=gte.{since.isoformat()}&select=keyword,region,traffic_volume&order=crawled_at.desc&limit=50"
    resp = requests.get(url, headers=get_supabase_headers())
    if resp.status_code == 200:
        return resp.json()
    return []


def generate_weekly_report(reddit_data, youtube_data, google_data):
    """3ê°œ í”Œë«í¼ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ AI ì£¼ê°„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""

    reddit_summary = "\n".join([
        f"- [{p.get('subreddit','')}] {p.get('title','')}"
        for p in reddit_data[:20]
    ]) or "ë°ì´í„° ì—†ìŒ"

    youtube_summary = "\n".join([
        f"- [{v.get('category','')}] {v.get('title','')} ({v.get('channel_title','')}, ì¡°íšŒìˆ˜: {v.get('view_count',0):,})"
        for v in youtube_data[:20]
    ]) or "ë°ì´í„° ì—†ìŒ"

    google_summary = "\n".join([
        f"- {k.get('keyword','')} ({k.get('region','')}: {k.get('traffic_volume','')})"
        for k in google_data[:30]
    ]) or "ë°ì´í„° ì—†ìŒ"

    system_prompt = "í•œêµ­ ìœ íŠœë²„ë¥¼ ìœ„í•œ íŠ¸ë Œë“œ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤."

    user_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ ìœ íŠœë²„ë¥¼ ìœ„í•œ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ 3ê°œ í”Œë«í¼ì—ì„œ ìˆ˜ì§‘í•œ ì´ë²ˆ ì£¼ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬, í•œêµ­ì–´ë¡œ ì£¼ê°„ íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

## ğŸ“Œ Reddit í¬ë¦¬ì—ì´í„° ì»¤ë®¤ë‹ˆí‹° í•« í† í”½
{reddit_summary}

## ğŸ“Œ YouTube ì¸ê¸° ë™ì˜ìƒ (í•œêµ­ + ë¯¸êµ­)
{youtube_summary}

## ğŸ“Œ Google ê²€ìƒ‰ íŠ¸ë Œë“œ
{google_summary}

---

ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

# ğŸ“Š ì´ë²ˆ ì£¼ í¬ë¦¬ì—ì´í„° íŠ¸ë Œë“œ ë¦¬í¬íŠ¸

## ğŸ”¥ HOT í‚¤ì›Œë“œ TOP 5
(3ê°œ í”Œë«í¼ì—ì„œ ê³µí†µì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ì£¼ì œ/í‚¤ì›Œë“œë¥¼ ë½‘ì•„ì£¼ì„¸ìš”)

## ğŸ“º ìœ íŠœë¸Œ ì½˜í…ì¸  ì œì•ˆ 3ê°€ì§€
(ê° ì œì•ˆë§ˆë‹¤: ì£¼ì œ, ì˜ˆìƒ íƒ€ì´í‹€, ì™œ ì§€ê¸ˆ ë§Œë“¤ì–´ì•¼ í•˜ëŠ”ì§€)

## ğŸŒ í•´ì™¸ vs í•œêµ­ íŠ¸ë Œë“œ ë¹„êµ
(ë¯¸êµ­ì—ì„œëŠ” ëœ¨ê³  ìˆì§€ë§Œ í•œêµ­ì—ì„œëŠ” ì•„ì§ ì•ˆ ë‹¤ë£¬ ì£¼ì œê°€ ìˆë‹¤ë©´)

## âš¡ ì´ë²ˆ ì£¼ ì•¡ì…˜ ì•„ì´í…œ
(ìœ íŠœë²„ê°€ ì§€ê¸ˆ ë‹¹ì¥ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ í–‰ë™ 3ê°€ì§€)"""

    report = call_openai(system_prompt, user_prompt)

    if report:
        hot_keywords = extract_hot_keywords(report)
        print("  âœ¨ ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
        return report, hot_keywords
    return None, None


def extract_hot_keywords(report_text):
    """ë¦¬í¬íŠ¸ì—ì„œ HOT í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    keywords = []
    in_hot_section = False
    for line in report_text.split("\n"):
        if "HOT í‚¤ì›Œë“œ" in line or "HOT" in line.upper():
            in_hot_section = True
            continue
        if in_hot_section:
            if line.startswith("##"):
                break
            cleaned = line.strip().lstrip("0123456789.-) ").strip()
            if cleaned and len(cleaned) > 1:
                cleaned = cleaned.replace("**", "").strip()
                if cleaned:
                    keywords.append(cleaned)
    return keywords[:10]


def save_report_to_supabase(report_content, hot_keywords, sources_summary=None):
    """ì£¼ê°„ ë¦¬í¬íŠ¸ë¥¼ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤."""
    if not SUPABASE_URL or not SUPABASE_KEY:
        return

    today = date.today()
    week_start = today - timedelta(days=today.weekday())

    data = {
        "week_start": week_start.isoformat(),
        "report_content": report_content,
        "hot_keywords": json.dumps(hot_keywords, ensure_ascii=False),
        "sources_summary": json.dumps(sources_summary or {}, ensure_ascii=False),
        "created_at": datetime.now().isoformat()
    }

    endpoint = f"{SUPABASE_URL}/rest/v1/weekly_reports"
    resp = requests.post(endpoint, json=data, headers=get_supabase_headers())

    if resp.status_code in range(200, 300):
        print(f"  ğŸ’¾ ì£¼ê°„ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ! (ì£¼ì°¨: {week_start})")
    else:
        print(f"  âŒ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {resp.status_code} - {resp.text[:200]}")


def run_trend_analysis():
    """ì „ì²´ êµì°¨ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print(f"\n[{datetime.now()}] ğŸ§  Cross-Platform Trend Analysis ì‹œì‘...")

    print("  ğŸ“¡ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    reddit_data = fetch_recent_reddit_posts()
    youtube_data = fetch_recent_youtube_trends()
    google_data = fetch_recent_google_trends()

    print(f"  ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: Reddit {len(reddit_data)}ê°œ, YouTube {len(youtube_data)}ê°œ, Google {len(google_data)}ê°œ")

    if not reddit_data and not youtube_data and not google_data:
        print("  âš ï¸ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ëŸ¬ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    print("  ğŸ¤– AI ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    report, hot_keywords = generate_weekly_report(reddit_data, youtube_data, google_data)

    if report:
        sources_summary = {
            "reddit_count": len(reddit_data),
            "youtube_count": len(youtube_data),
            "google_count": len(google_data),
            "analysis_date": datetime.now().isoformat()
        }
        save_report_to_supabase(report, hot_keywords, sources_summary)

        print("\n" + "=" * 60)
        print("ğŸ“‹ ì£¼ê°„ ë¦¬í¬íŠ¸ ë¯¸ë¦¬ë³´ê¸°:")
        print("=" * 60)
        print(report[:800] + "..." if len(report) > 800 else report)
        print("=" * 60)
    else:
        print("  âŒ ë¦¬í¬íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    print("  ğŸ§  Cross-Platform Trend Analysis ì™„ë£Œ!\n")


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--once":
        print("ğŸš€ Trend Analyzer (ë‹¨ì¼ ì‹¤í–‰)...")
        run_trend_analysis()
        print("âœ… ì™„ë£Œ!")
    else:
        import time
        print("ğŸš€ Trend Analyzer (ë°˜ë³µ ëª¨ë“œ, 24ì‹œê°„ ê°„ê²©)")
        while True:
            run_trend_analysis()
            print("ğŸ˜´ 24ì‹œê°„ ëŒ€ê¸° ì¤‘...")
            time.sleep(86400)
