"""
YouTube Trending Crawler - The Info Club v2.0
YouTube Data APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­/ë¯¸êµ­ ì¸ê¸° ë™ì˜ìƒì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""
import os
import requests
from dotenv import load_dotenv
from datetime import datetime, date

load_dotenv()

YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# YouTube ì¹´í…Œê³ ë¦¬ ID â†’ ì´ë¦„ ë§¤í•‘ (ì£¼ìš” ì¹´í…Œê³ ë¦¬)
CATEGORY_MAP = {
    "1": "ì˜í™”/ì• ë‹ˆë©”ì´ì…˜", "2": "ìë™ì°¨", "10": "ìŒì•…",
    "15": "ë™ë¬¼", "17": "ìŠ¤í¬ì¸ ", "19": "ì—¬í–‰/ì´ë²¤íŠ¸",
    "20": "ê²Œì„", "22": "ì¼ìƒ/ë¸”ë¡œê·¸", "23": "ì½”ë¯¸ë””",
    "24": "ì—”í„°í…Œì¸ë¨¼íŠ¸", "25": "ë‰´ìŠ¤/ì •ì¹˜", "26": "ìŠ¤íƒ€ì¼",
    "27": "êµìœ¡", "28": "ê³¼í•™/ê¸°ìˆ ", "29": "ë¹„ì˜ë¦¬/ì‚¬íšŒìš´ë™"
}

# ìˆ˜ì§‘ ëŒ€ìƒ ì§€ì—­
TARGET_REGIONS = ["KR", "US"]


def get_supabase_headers():
    return {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json",
        "Prefer": "resolution=merge-duplicates"
    }


def fetch_trending_videos(region_code="KR", max_results=25):
    """
    YouTube Data APIë¡œ íŠ¹ì • ì§€ì—­ì˜ ì¸ê¸° ë™ì˜ìƒì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        region_code: êµ­ê°€ ì½”ë“œ (KR, US ë“±)
        max_results: ê°€ì ¸ì˜¬ ë™ì˜ìƒ ìˆ˜ (ìµœëŒ€ 50)
    
    Returns:
        list of video data dicts
    """
    if not YOUTUBE_API_KEY:
        print("  âš ï¸ YOUTUBE_API_KEY not set, skipping YouTube trending.")
        return []

    url = "https://www.googleapis.com/youtube/v3/videos"
    params = {
        "part": "snippet,statistics",
        "chart": "mostPopular",
        "regionCode": region_code,
        "maxResults": max_results,
        "key": YOUTUBE_API_KEY
    }

    try:
        response = requests.get(url, params=params)
        if response.status_code != 200:
            print(f"  âŒ YouTube API Error ({region_code}): {response.status_code} - {response.text[:200]}")
            return []

        data = response.json()
        videos = []

        for item in data.get("items", []):
            snippet = item.get("snippet", {})
            stats = item.get("statistics", {})
            category_id = snippet.get("categoryId", "0")

            videos.append({
                "video_id": item["id"],
                "title": snippet.get("title", ""),
                "channel_title": snippet.get("channelTitle", ""),
                "category": CATEGORY_MAP.get(category_id, f"ê¸°íƒ€({category_id})"),
                "view_count": int(stats.get("viewCount", 0)),
                "like_count": int(stats.get("likeCount", 0)),
                "comment_count": int(stats.get("commentCount", 0)),
                "region": region_code,
                "thumbnail_url": snippet.get("thumbnails", {}).get("high", {}).get("url", ""),
                "trending_date": date.today().isoformat(),
                "crawled_at": datetime.now().isoformat()
            })

        print(f"  âœ… {region_code}: {len(videos)}ê°œ ì¸ê¸° ë™ì˜ìƒ ìˆ˜ì§‘ ì™„ë£Œ")
        return videos

    except Exception as e:
        print(f"  âŒ YouTube API Exception ({region_code}): {e}")
        return []


def save_to_supabase(videos):
    """ìˆ˜ì§‘í•œ ë™ì˜ìƒ ë°ì´í„°ë¥¼ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤."""
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("  âš ï¸ Supabase credentials not set.")
        return

    endpoint = f"{SUPABASE_URL}/rest/v1/youtube_trends?on_conflict=video_id"
    headers = get_supabase_headers()

    saved = 0
    for video in videos:
        try:
            resp = requests.post(endpoint, json=video, headers=headers)
            if resp.status_code in range(200, 300):
                saved += 1
            else:
                print(f"  âš ï¸ Save failed for {video['title'][:30]}: {resp.status_code}")
        except Exception as e:
            print(f"  âŒ Save error: {e}")

    print(f"  ğŸ’¾ {saved}/{len(videos)}ê°œ ì €ì¥ ì™„ë£Œ")


def run_youtube_crawler():
    """YouTube íŠ¸ë Œë”© í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    print(f"\n[{datetime.now()}] ğŸ¬ YouTube Trending Crawler ì‹œì‘...")

    all_videos = []
    for region in TARGET_REGIONS:
        print(f"  ğŸ“ {region} ì§€ì—­ ì¸ê¸° ë™ì˜ìƒ ìˆ˜ì§‘ ì¤‘...")
        videos = fetch_trending_videos(region_code=region)
        all_videos.extend(videos)

    if all_videos:
        save_to_supabase(all_videos)
    else:
        print("  âš ï¸ ìˆ˜ì§‘ëœ ë™ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")

    print(f"  ğŸ¬ YouTube Trending Crawler ì™„ë£Œ! (ì´ {len(all_videos)}ê°œ)\n")
    return all_videos


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--once":
        print("ğŸš€ YouTube Trending Crawler (ë‹¨ì¼ ì‹¤í–‰)...")
        run_youtube_crawler()
        print("âœ… ì™„ë£Œ!")
    else:
        import time
        print("ğŸš€ YouTube Trending Crawler (ë°˜ë³µ ëª¨ë“œ, 30ë¶„ ê°„ê²©)")
        while True:
            run_youtube_crawler()
            print("ğŸ˜´ 30ë¶„ ëŒ€ê¸° ì¤‘...")
            time.sleep(1800)  # 30ë¶„ ê°„ê²© (API í• ë‹¹ëŸ‰ ì ˆì•½)
